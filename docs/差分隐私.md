# 熵

## 信息量

信息量度量的是一个具体事件发生所带来的信息。

定义如下：
$$
h(x)=-\log _{2} p(x)
$$
信息量的大小和事件发生的概率成反比。

## 香农熵

香农熵是用来衡量信息不确定性的指标，不确定性是一个事件出现不同结果的可能性。
$$
H[x]=E_{x \sim p}[h(x)]=-E_{x \sim p}[\log p(x)]=-\sum_{x} p(x) \log _{2} p(x)
$$
![](https://gitee.com/wardseptember/images/raw/master/imgs/20201002171621.png)

香农熵也即信息熵，表示编码方案完美时，最短平均编码长度是多少。

## 交叉熵

交叉熵指的是编码方案不一定完美时，平均编码长度是多少。

现有关于样本的两个概率分布$p$和$q$，其中$p$为真实分布，$q$为非真实分布。如果使用错误分布$q$来表示来自真实分布$p$的平均编码长度，则应该是：
$$
H(p, q)=-\sum_{i=0}^{n} p\left(x_{i}\right) \log \left(q\left(x_{i}\right)\right)
$$

## 相对熵

由$q$得到的平均编码长度比由$p$得到的平均编码长度多出的位数称为“相对熵”，也叫做KL散度。KL散度用于衡量两个概率分布之间的差异，差异越大则相对熵越大。
$$
D_{K L}(p \| q)=H(p, q)-H(p)=\sum_{i=0}^{n}[p(x_i) *(\log p(x_i)-\log q(x_i))]
$$
编码方案不一定完美时，平均编码长度相对于最小值的增加值。

> 相对熵=交叉熵-信息熵

## 任意熵

任意熵是香农熵、Hartley熵和最小熵等的一般形式。
$$
\mathrm{H}_{\alpha}(x)=\frac{1}{1-\alpha} \log \left(\sum_{i=1}^{n} [p(x_i)]^{\alpha}\right)
$$
其中$\alpha \geq 0$ 和 $\alpha \neq 1$。

### Hartley熵

当$\alpha=0$时，$\mathrm{H}_{0}(x)$实际上就是hartley熵
$$
\mathrm{H}_{0}(x)=\frac{1}{1-0} \log \left(\sum_{i=1}^{n} p(x_i)^{0}\right)=\log n
$$

### 香农熵

当$\alpha \rightarrow 1$时，$\mathrm{H}_{1}(X)$实际上就是香农熵。
$$
\mathrm{H}_{1}(X)=\lim _{\alpha \rightarrow 1} \frac{1}{-1} \frac{\sum_{i=1}^{n} p(x_i)^{\alpha} \ln p(x_i)}{\left(\sum_{i=1}^{n} p(x_i)^{\alpha}\right)}=-\sum_{i=1}^{n} p(x_i) \ln p(x_i)
$$

## Renyi Divergence

任意散度用于衡量两个概率分布之间的差异。`Renyi Divergence`也是`KL-Divergence`和`Max-Divergence`的推广。
$$
D_{\alpha}(P \| Q)=\frac{1}{\alpha-1} \log \left(\sum_{i=1}^{n} q(x_i) \frac{p(x_i)^{\alpha}}{q(x_i)^{\alpha}}\right)
$$
其中，$P$和$Q$分别表示两个随机变量，且其概率分布分别为$𝑝(𝑥)$、$𝑞(𝑥)$

### KL散度

当$\alpha \rightarrow 1$时，任意散度就是KL散度
$$
D_{1}(P \| Q)=\lim _{\alpha \rightarrow 1} \frac{1}{\alpha-1} \log \left(\sum_{i=1}^{n} q_{i} \frac{p_{i}^{\alpha}}{q_{i}^{\alpha}}\right)=\sum_{i=1}^{n} p_{i} \log \frac{p_{i}}{q_{i}}
$$

### Max Divergence

当$\alpha \rightarrow \infty$时，任意散度就是Max Divergence
$$
D_{\infty}(P \| Q)=\lim _{\alpha \rightarrow \infty} \frac{1}{\alpha-1} \log \left(\sum_{i=1}^{n} q_{i} \frac{p_{i}^{\alpha}}{q_{i}^{\alpha}}\right)=\log \max _{i} \frac{p_{i}}{q_{i}}
$$

# Differential privacy

## 标准差分隐私

A randomized mechanism M: D → R with domain D and range R satisfies (ε,δ)-differential privacy if for any two adjacent inputs d, d′ ∈ D and for any subset of outputs S ⊆ R it holds that
$$
\operatorname{Pr}[\mathcal{M}(d) \in S] \leq e^{\varepsilon} \operatorname{Pr}\left[\mathcal{M}\left(d^{\prime}\right) \in S\right]+\delta
$$

$$
\mathcal{M}(d) \triangleq f(d)+\mathcal{N}\left(0, S_{f}^{2} \cdot \sigma^{2}\right)
$$

其中
$$
S_{f} = max(\left|f(d)-f\left(d^{\prime}\right)\right|)
$$


Let $\varepsilon \in(0,1)$ be arbitrary. For $c^{2}>2 \ln (1.25 / \delta),$ the Gaussian Mechanism with parameter $\sigma \geq c \Delta_{2} f / \varepsilon$ is $(\varepsilon, \delta)$ -differentially private.

证明：

### 高斯分布

若随机变量 $X$服从均值为 $μ$，方差为$\sigma^{2}$的正态分布，记为:
$$
X \sim \mathcal{N}\left(\mu, \sigma^{2}\right)
$$
其概率密度函数为：
$$
f(x)=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}
$$


* L1范数

    假设X是n维的向量$X=\left(x_{1}, x_{2}, x_{3}, \ldots x_{n}\right)$，$\|X\|_{1}=\sum_{i=1}^{n} |x_{i}|$

* L2范数

    假设X是n维的向量$X=\left(x_{1}, x_{2}, x_{3}, \ldots x_{n}\right)$，$\|X\|_{2}=\sqrt{\sum_{i=1}^{n} x_{i}^{2}}$

L2范数
$$
\Delta_{2} f=\max _{\mathrm{adjacent} x, y}\|f(x)-f(y)\|_{2}
$$
假设$f(x)$是实值函数，则$\Delta f=\Delta_{1} f=\Delta_{2} f$。

差分隐私定义可以化为：
$$
|\ln \frac{\operatorname{Pr}[\mathcal{M}(d) \in S]}{\operatorname{Pr}[\mathcal{M}(d^{\prime}) \in S]}| \leq \varepsilon
$$

$$
|\ln \frac{e^{\left(-1 / 2 \sigma^{2}\right) x^{2}}}{e^{\left(-1 / 2 \sigma^{2}\right)(x+\Delta f)^{2}}}| = \mid \frac{1}{2 \sigma^{2}}\left(2 x \Delta f+(\Delta f)^{2}\right)| \leq \varepsilon
$$

$$
x<\sigma^{2} \frac{\varepsilon}{\Delta f}-\Delta f / 2
$$

![](https://gitee.com/wardseptember/images/raw/master/imgs/20201003222026.png)

![](https://gitee.com/wardseptember/images/raw/master/imgs/20201003222103.png)

![](https://gitee.com/wardseptember/images/raw/master/imgs/20201003222125.png)

![](https://gitee.com/wardseptember/images/raw/master/imgs/20201003222158.png)

## 任意差分隐私

### 任意散度

Let $P$ and $Q$ be two distributions on $\mathcal{X}$ defined over the same probability space, and let p and q be their respective densities. The Rényi divergence of a finite order $\alpha \neq 1$ between $P$
and $Q$ is defined as：
$$
\mathrm{D}_{\alpha}(P \| Q) \triangleq \frac{1}{\alpha-1} \ln \int_{\mathcal{X}} q(x)\left(\frac{p(x)}{q(x)}\right)^{\alpha} \mathrm{d} x
$$

### Re ́nyi differential privacy(RDP)

We say that a randomized mechanism $\mathcal{M}: \mathcal{S} \rightarrow \mathcal{R}$ satisfies $(\alpha, \varepsilon)-$Rényi differential privacy $(R D P)$ if for any two adjacent inputs $S, S^{\prime} \in \mathcal{S}$ it holds that:
$$
\mathrm{D}_{\alpha}\left(\mathcal{M}(S) \| \mathcal{M}\left(S^{\prime}\right)\right) \leq \varepsilon
$$
其中$S^{\prime}=S \cup\{x\}$

