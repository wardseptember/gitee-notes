# 集成学习

## 决策树

![](http://wardseptember.top/20200705151604.png)

## 分类树

### 信息熵

信息熵是用来衡量信息不确定性的指标，不确定性是一个事件出现不同结果的可能性。计算方法如下：

![](http://wardseptember.top/20200705152115.png)

#### 条件熵和信息增益

![](http://wardseptember.top/20200705152312.png)

信息增益越大，划分就越好，信息不确定性降低，说明结果越可靠。

![](http://wardseptember.top/20200705152610.png)

### 基尼指数

基尼指数（Gini不纯度）表示在样本集合中一个随机选中的样本被分错的概率。

![](http://wardseptember.top/20200705153959.png)



Gini增益跟信息增益类似，是父节点的基尼指数-子节点的基尼指数。

使用基尼指数进行划分得到是一个CART树（二叉树）。

![](http://wardseptember.top/20200705154632.png)

## 回归树

![](http://wardseptember.top/20200705154805.png)

![](http://wardseptember.top/20200705154936.png)

## 集成学习简介

![](http://wardseptember.top/20200705155604.png)

### Bagging

![](http://wardseptember.top/20200705155801.png)

每次随机选择m个样本，训练一个分类器hi，循环这个过程n次，得到n个分类器，然后用这个n个分类器分别预测结果，选择预测结果相同且出现次数最多的结果作为最终结果。

### Boosting

![](http://wardseptember.top/20200705160245.png)

![](http://wardseptember.top/20200705160443.png)

### 随机森林

随机森林是bagging算法里面的代表。

![](http://wardseptember.top/20200705160612.png)

![](http://wardseptember.top/20200705160901.png)

### Adaboost

![](http://wardseptember.top/20200705161634.png)

算法流程

![](http://wardseptember.top/20200705161920.png)

![](http://wardseptember.top/20200705162215.png)

![](http://wardseptember.top/20200705162445.png)

![](http://wardseptember.top/20200705162617.png)

![](http://wardseptember.top/20200705162735.png)

![](http://wardseptember.top/20200705162826.png)

![](http://wardseptember.top/20200705163005.png)

![](http://wardseptember.top/20200705163048.png)

### GDBT

![](http://wardseptember.top/20200705163556.png)

用弱学习器训练结果预测原数据集的标签，然后与原数据集的标签做差得到残差，然后用残差训练另一个弱学习器，重复此过程，直到收敛，然后聚合所有弱学习器得到最终的强学习器。

![](http://wardseptember.top/20200705164519.png)

![](http://wardseptember.top/20200705164756.png)

![](http://wardseptember.top/20200705165000.png)

代替残差，用负梯度拟合基学习器。梯度方向，增加最大，负梯度方向，减小最大。

![](http://wardseptember.top/20200705165110.png)

![](http://wardseptember.top/20200705165352.png)

![](http://wardseptember.top/20200705165519.png)

![](http://wardseptember.top/20200705165707.png)

![](http://wardseptember.top/20200705165805.png)

![](http://wardseptember.top/20200705165908.png)

![](http://wardseptember.top/20200705165949.png)

### XGBoost

![](http://wardseptember.top/20200705170241.png)

#### 模型形式

![](http://wardseptember.top/20200705170405.png)

![](http://wardseptember.top/20200705170527.png)

![](http://wardseptember.top/20200705170604.png)

#### 目标函数

![](http://wardseptember.top/20200705170656.png)

前面t-1的值已经确定，最小化$f_t(x_i)$就可以得到正则化项最小值。

![](http://wardseptember.top/20200705170856.png)

![](http://wardseptember.top/20200705172152.png)

![](http://wardseptember.top/20200705172300.png)

![](http://wardseptember.top/20200705172428.png)

![](http://wardseptember.top/20200705172549.png)

![](http://wardseptember.top/20200705172651.png)

![](http://wardseptember.top/20200705172803.png)

![](http://wardseptember.top/20200705172857.png)

![](http://wardseptember.top/20200705173035.png)

$g_1$是一阶导，$h_1$是二阶导

![](http://wardseptember.top/20200705173215.png)

#### 学习策略

![](http://wardseptember.top/20200705173428.png)

找到增益最大

![](http://wardseptember.top/20200705173707.png)

![](http://wardseptember.top/20200705173829.png)

精确贪心算法时间复杂度高，用下面的近似算法替代。

![](http://wardseptember.top/20200705173854.png)

![](http://wardseptember.top/20200705174204.png)

![](http://wardseptember.top/20200705174331.png)

![](http://wardseptember.top/20200705174439.png)

![](http://wardseptember.top/20200705174512.png)

![](http://wardseptember.top/20200705174647.png)

![](http://wardseptember.top/20200705174730.png)

![](http://wardseptember.top/20200705174826.png)

![](http://wardseptember.top/20200705174848.png)

#### 系统设计

![](http://wardseptember.top/20200705174958.png)

![](http://wardseptember.top/20200705175107.png)

![](http://wardseptember.top/20200705175148.png)

![](http://wardseptember.top/20200705175220.png)

![](http://wardseptember.top/20200705175243.png)

![](http://wardseptember.top/20200705175310.png)

