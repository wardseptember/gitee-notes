# 论文阅读

## Federated Machine Learning: Concept and Applications

简而言之，联邦学习是在保护用户数据隐私的前提下，进行机器学习。

联邦学习分为三类，分别是横向联邦学习、纵向联邦学习和联邦迁移学习。

* 横向联邦学习

    当用户Id重叠少，用户特征重叠多，这种情况适合用横向联邦学习。举个例子，不同地区的两个银行，虽然他们的用户不一样，但是他们的业务大致相同。

    ![](http://wardseptember.top/20200630175447.png)

* 纵向联邦学习

    当用户Id重叠多，用户特征重叠少，这种情况适合用纵向联邦学习。比如在同一个地区的超市和银行，他们的客户大致相同，但是服务的业务不同。

    ![](http://wardseptember.top/20200630175718.png)

* 联邦迁移学习

    当用户Id重叠少，用户特征也重叠少，这种情况适合用联邦迁移学习。

    ![](http://wardseptember.top/20200630180051.png)

    

### 联邦学习的实现架构

* 横向联邦学习

    ![](http://wardseptember.top/20200630180247.png)

    客户端B1、B2、Bk本地计算梯度，然后发送给服务器A加权聚合，将聚合后参数分别发送给B1、B2、Bk更新模型，然后重复此过程，直至模型收敛。

    训练结束，整个聚合后的模型和整个模型参数是被所有参与方共享的。

* 纵向联邦学习

    ![](http://wardseptember.top/20200630180756.png)

    纵向联邦学习也是数据不出本地，本地计算梯度和损失函数，然后发送给中间方C，C主要负责加密和解密工作，将解密后的结果发送给A和B。注意：A、B向C发送梯度或者参数时需要加密。A和B之间不能直接交换数据，交换的是中间结果(梯度、目标函数等)。而且只有一方有标签数据(在这里就是B)。A和B得到的也只是自己特征的参数更新，并不能得到彼此的参数，这也是跟横向联邦的一个区别，因此在预测推理时需要两方协作计算预测结果。

* 联邦迁移学习

    联邦迁移学习的实现框架跟纵向联邦类似，A和B重叠的特征和用户少，需要迁移学习来扩展至整个数据集。迁移学习引入的目的是减少预测时的错误，因此在梯度计算时和纵向联邦略有不同。

## A Communication-Efficient Collaborative Learning Framework for Distributed Features

这篇论文介绍了一个高效通信协作学习框架，属于纵向联邦学习范畴。

提出了FedBCD-p和FedBCD-s算法，有效地减少了客户端之间的交流次数，比起FedSGD这两个算法有更高的效率。这两个算法跟FedAvg算法类似，都是在本地更新Q轮参数，然后再跟带标签的客户端交互，他们之间的区别是：

* FedAvg是横向联邦学习聚合里面的算法，需要将所有梯度收集起来然后加权平均，再发给客户机
* FedBCD是纵向联邦学习里面的算法，客户机之间交互时，并不会把不属于当前机器的特征参数发过来。

同时这篇论文证明，模型精度跟本地更新参数轮数Q有关，Q取太大精度会降低，Q取太少通信成本又高了，实践表明当Q取大一点的值时FedPBCD-p算法精度比上面两个精度高。

`FedPBCD-p`是运用Proximal Gradient Descent思想限制本地更新靠近初始模型参数，详见下图：

![](http://wardseptember.top/20200701115634.png)

## CatBoost: unbiased boosting with categorical features

CatBoost是一个无偏的，分类问题梯度增强算法。之前的梯度增强算法，如XGBoost、LightGBM，都有一个`target leakage`和`prediction shift`的问题。catboost解决了这两个问题。



## Advances and Open Problems in Federated Learning

### Relaxing the Core FL Assumptions: Applications to Emerging Settings and Scenarios

#### 完全去中心化/点对点分布式学习

在联邦学习中有一个中心服务器组织训练过程，这可能导致单点故障和系统性能瓶颈。

完全去中心话分布式学习是点对点交流，可以免去一个中心服务器。

##### 算法挑战

去中心化SGD算法可能面临的问题是网络延迟等。

了解非IID数据分布下的收敛性以及如何设计实现最快收敛性的模型均衡策略仍然是一个悬而未决的问题。

完全去中心化联邦学习其中一个挑战仍然是鲁棒性。

无损的梯度压缩算法是一个可行的研究方向。

##### 实践挑战

如何实现一个完全的去中心化联邦学习。

用区块链这种技术实现完全去中心化联邦学习，不过还要进行一些数据加密。通过全局聚合模型中添加高斯噪声，实现数据传输加密。

#### 跨库联邦学习

跨库联邦学习在整体设计的某些方面拥有更大的灵活性，但在实现其他属性上可能更加困难。

##### 数据分区

联邦迁移学习用于只要一小部分数据重叠的情况，但现在的公式仅适用于两个客服端的情况。

##### 激励机制

Incentive mechanism design for honest participation is an important practical research question.

相关目标包括如何将联邦学习模型产生的收益分配给贡献数据的所有者以维持长期参与，如何将激励措施与防御对抗性数据所有者的决策联系起来以增强系统安全性，以及优化数据所有者的参与以提高系统效率。

##### 差异隐私

对于具有说服力的主张，我们通常需要本地差异隐私的概念，因为来自其他客户的潜在威胁可能更加重要。

##### 张量分解

一些人工作还研究了跨仓库联合张量分解，其中多个站点（每个站点都有一组具有相同特征的数据，即水平分区）通过仅与协调服务器共享中间因子来共同执行张量分解，同时保持每个站点的数据私有

#### Split Learning

拆分学习是指客户端和服务器之间逐层拆分模型的执行。

关键思想是最大程度地减少原始数据和传输数据之间的相关性。

### 提高效率和效果

在本节中，我们将探讨各种技术和未解决问题，以解决使联邦学习更加高效和有效的挑战。

#### 联邦学习中非独立同分布数据(Non-IID)

如何处理非独立同分布数据仍然是一个悬而未决的问题。

时差对构建联邦学习模型的影响未解决。

Adapting techniques for handling dataset shift to federated learning is another interesting open question.

##### 处理非独立同分布数据的策略

训练一个全局模型是困难的。尝试修改现存的算法或者研发一个新算法去处理Non-IID是一个不错的方向。

扩展数据使数据在特征上更相似。

为每一个用户训练一个本地模型。

#### 优化联邦学习算法





## FEDERATED LEARNING: STRATEGIES FOR IMPROVING COMMUNICATION EFFICIENCY

联邦学习中，客户端要向服务器发送更新后的参数，如果模型很大，参数自然也很大，这篇论文主要是提出减少客户端向服务器上传参数文件的代价的两种方法，一是`Structured updates`，而是`Sketched updates`。

* `Structured updates`，是直接在有限的空间中更新参数，用一个较小的变量集确定参数。
* `Sketched updates`，是全局模型更新，但是发送给服务器之前先压缩参数文件。

## Communication-Efficient Learning of Deep Networks from Decentralized Data

这篇论文第一次提出联邦学习的概念。

>We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach **Federated Learning**.

这篇论文主要区分FedSGD和FedAvg的概念。

几个变量：

* C，是从所有客户端中选择本轮更新迭代的客户端数量。假设客户端总量为n，本轮从中随机选择k个客户端进行训练。(k大于等于2小于等于n)
* E，是epochs，即数据被“轮”了多少次
* B，是batch size，从本地数据集中选择B个样本进行训练

### FedSGD

FederatedSGD (or FedSGD)是指只有一个batch(a single batch)和Epoch数为1的算法。
令`B`和`E`分别对应与本地客户(lcoal client)的Batch-Size和Epoch数，那么FedSGD对应于`B=∞`和`E=1`。

直观理解：传统的SGD算法可以看成是梯度更新速度最快的优化算法，因为每一个sample就更新一次梯度了，这里也是一样，FedSGD是应用FL时梯度或模型更新速度最快的算法，它只要求每个local client计算一次平均梯度就可以上传到central server进行加权平均了，所以它需要的**computation power**是最少的(the lowest)。

> 本文作者是想研究如何通过利用额外的计算容量来降低通信损失，而无疑基线就是计算容量最小的`FedSGD`算法了。

### FedAvg

只要`B≠∞`和`E≠1`，那么此时的算法就叫做`FedAvg`。

`FedAvg(FederatedAveraging )`算法是指local client先在本地计算多次梯度并且更新权值，这时的计算成本是提升的。

FedSGD是上传梯度，然后中心服务器更新权重；FedAvg是本地计算梯度后，本地更新权重，然后将权重上传到中心服务器。这两种是等价的方式，见下图。

![](http://wardseptember.top/20200630145427.png)

FedAvg提出的意义和重点如下:

![](http://wardseptember.top/20200630145836.png)

FedAvg伪代码如下：

![](http://wardseptember.top/20200630145529.png)

#### 参考链接：

* [https://www.jianshu.com/p/8ced840f07fe](https://www.jianshu.com/p/8ced840f07fe)

## 同态加密

同态加密是一种加密技术，允许直接在密文上计算，生成加密结果，当加密结果被解密时，其结果与直接在明文上计算的结果一致。

同态是指代数中的同态：加密和解密功能可以认为是纯文本空间和密文空间之间的同构。

### Paillier cryptosystem

* 非对称加密算法

[https://en.wikipedia.org/wiki/Paillier_cryptosystem](https://en.wikipedia.org/wiki/Paillier_cryptosystem)

## FEDERATED LEARNING FOR MOBILE KEYBOARD PREDICTION

这篇论文用LSTM建议一个语言模型，用于预测用户接下来要输入的内容。横向联邦，数据不出本地，采用FedAvg的方法更新权重，所有用户共享一个模型。

## NGBoost

### KL散度

KL散度是衡量新分布和旧分布之间差异的变量。

![](http://wardseptember.top/20200728144210.png)

![](http://wardseptember.top/20200728144244.png)

![](http://wardseptember.top/20200728144319.png)

![](http://wardseptember.top/20200728144359.png)

![](http://wardseptember.top/20200728144437.png)

### 自然梯度

![](http://wardseptember.top/20200729154906.png)

![](http://wardseptember.top/20200729143214.png)

Fisher Information Matrix (FIM)就是KL散度的二阶近似值，也是log-p的Hessian矩阵的期望。

![](http://wardseptember.top/20200729143451.png)

![](http://wardseptember.top/20200729143526.png)

自然梯度就是一个无穷小量乘FIM矩阵的逆再乘目标函数的梯度。

#### 参考链接

https://www.zhihu.com/question/266846405/answer/314354512

https://zhuanlan.zhihu.com/p/82934100



## LoAdaBoost:Loss-Based AdaBoost Federated Machine Learning on medical data



![](http://wardseptember.top/20200804223501.png)

这篇论文主要介绍了一种自适应联邦平均方法，每次取E/2 epochs的Loss Function 值作为参照物，如果某个客户端Loss值小于平均Loss值就不再训练，直接发送给服务端；如果某个客户端Loss值大于平均Loss值就继续训练。

## **Deep Learning with Differential Privacy**

满足下面的条件就符合差分隐私理论。

![](http://wardseptember.top/20200805212704.png)

![](http://wardseptember.top/20200813202740.png)

* l2_norm_clip：l2范数 : $\|X\|_{2}=\sqrt{\sum_{i=1}^{n} x_{i}^{2}}$，上图中的C

    用于裁剪梯度，按照经验设置为0.5~1.0之间有比较好的效果

* noise_multiplier：正态分布中的$\sigma$，正态分布用于向梯度中添加噪声保证隐私数据安全，最小设置为0.3

* num_microbatches：将batch分成多少个num_microbatches

$\epsilon$对通过包含（或删除）单个训练示例可以增加特定输出的概率进行了上限。 我们通常希望它是一个小的常数（小于10，或者，对于更严格的隐私保护，小于1）。

$δ$限制了模型行为任意改变的可能性。 我们通常可以将其设置为一个很小的数字（大约1e-7），而不会影响使用。 按照经验，是将其设置为小于训练数据大小的倒数。



## Differentially Private Federated Learning: A Client Level Perspective

![](http://wardseptember.top/20200807225133.png)

手写数字数据集先排序，然后给每个客户端最多分两个数字的子数据集，这样做，客户端的数据就是非独立同分布的了。

## L1和L2详解

[L1和L2 详解(范数、损失函数、正则化)](https://zhuanlan.zhihu.com/p/137073968)

L1和L2都可以做损失函数或者正则项，**用 L2 一定只有一条最好的预测线，L1 则因为其性质可能存在多个最优解**。

当然 L1 损失函数难道就没有什么好处了吗，也是有的，那就是**鲁棒性 (Robust) 更强，对异常值更不敏感**。

[L1和L2的特点](https://www.zhihu.com/question/26485586)

## 差分隐私

### 差分隐私简介

差分隐私可以确保得出相同的结论，例如，吸烟导致癌症，而不受个人选择进入或退出数据集的影响。

假设调查50名吸烟者，得出吸烟患肺癌的概率为80%，往这群吸烟者中再加入1个吸烟导致患肺癌的患者，不能因此得出吸烟患癌率为80.3%，因为这样会泄露最后一个人的隐私，可以确定他患肺癌。差分隐私的作用就是，往这群烟民中加入一些数据，并不能影响先前确定的结论，目的就是无法让外界判断新加入数据的信息。

$\varepsilon $越小，隐私保护的越好，损失的信息也越多。

差分隐私是一个定义，不是一个算法。

### 基本术语



Ray

# LDP-Fed: Federated Learning with Local Differential Privacy

本地差分隐私，横向联邦

重要点：

(1) the Local Differential Privacy Module running on each of the N clients and (2) the k-Client Selection Module.

发送参数给参数服务器(即中心服务器)之前，先在参数里面添加噪声，然后随机选择k个客户端的参数，进行更新。

![](https://gitee.com/wardseptember/images/raw/master/imgs/20200929213425.png)

这篇论文我觉得比较新的点是，使用一个叫CLDP的差分隐私算法，他与LDP的区别我还要再看看论文。

![](https://gitee.com/wardseptember/images/raw/master/imgs/20200929214411.png)

